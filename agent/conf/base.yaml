# Deep Research Agent Configuration
# This is the comprehensive base configuration for the multi-agent research system.
# Only includes parameters that are actually used in the codebase.

# Model Configuration - 4-Tier System with Rate Limiting
models:
  default:
    endpoint: databricks-gpt-oss-20b
    temperature: 0.7
    max_tokens: 16000
    reasoning_effort: low

  # TIER 1: MICRO - Ultra-lightweight (pattern matching, entity extraction)
  micro:
    endpoints:
      - databricks-gpt-oss-20b  # Highest priority (most throughput)
      - databricks-gemma-3-12b
    temperature: 0.5
    max_tokens: 4000
    tokens_per_minute: 200000  # Higher limit for lightweight model
    rotation_strategy: priority  # CHANGED: Priority-based (config order = priority order)
    fallback_on_429: true  # Single endpoint

  # TIER 2: SIMPLE - Lightweight (query gen, claim extraction, validation)
  simple:
    endpoints:
      - databricks-gpt-oss-20b
      - databricks-llama-4-maverick
    temperature: 0.5
    max_tokens: 8000
    reasoning_effort: low
    tokens_per_minute: 200000  # Medium rate limit
    rotation_strategy: priority  # CHANGED: Priority-based (config order = priority order)
    fallback_on_429: true  # Single endpoint

  # TIER 3: ANALYTICAL - Medium (research synthesis, fact checking, planning)
  analytical:
    endpoints:
      - databricks-gpt-oss-120b
      - databricks-gpt-oss-20b
    temperature: 0.7
    max_tokens: 12000
    reasoning_effort: medium
    tokens_per_minute: 200000  # Provisioned throughput limit
    rotation_strategy: priority  # CHANGED: Priority-based (config order = priority order)
    fallback_on_429: true  # Enable fallback to second endpoint

  # TIER 4: COMPLEX - Heavy (report generation, complex tables, synthesis)
  complex:
    endpoints:
      - databricks-gpt-oss-120b
      - databricks-claude-3-7-sonnet
      - databricks-claude-sonnet-4
    temperature: 0.7
    max_tokens: 25000  # Matches Databricks API limit for gpt-oss-120b
    reasoning_effort: high
    reasoning_budget: 8000
    tokens_per_minute: 50000  # Higher PT limit for powerful model
    rotation_strategy: priority  # CHANGED: Priority-based (config order = priority order)
    fallback_on_429: true  # Single powerful endpoint



# Multi-Agent System Configuration
multi_agent:
  enabled: true

agents:
  coordinator:
    enabled: true
    model: simple  # Use simple model for classification
    enable_safety_filter: true

  planner:
    enabled: true
    model: analytical  # Use analytical model for planning
    max_iterations: 2
    quality_threshold: 0.5
    enable_deep_thinking: false

  researcher:
    enabled: true
    model: analytical  # Use analytical model for research
    max_steps_per_execution: 20  # Increased for complex multi-country comparisons
    enable_reflexion: true
    # Smart Web Content Fetching - Selectively fetch full pages based on snippet analysis
    web_content_fetching:
      enabled: true
      min_pages_to_fetch: 8  # Guaranteed minimum URLs to fetch (increased for better quality)
      max_pages_to_fetch: 15  # Maximum URLs to fetch full content from (increased)
      top_results_to_consider: 15  # Pool size for selecting guaranteed minimum (matched to max)
      early_stop_threshold: 3  # Stop after N high-quality full pages
      early_stop_data_density: 0.6  # Minimum data density to consider stopping
      fetch_score_threshold: 0.4  # Minimum score (0-1) to fetch a URL (lowered to fetch more)
      dedup_snippets: true  # Deduplicate snippets before fetching
      dedup_urls: true  # Deduplicate URLs before fetching
      max_per_domain: 2  # Maximum fetches per domain for diversity
      timeout_seconds: 10  # Timeout for each page fetch
      max_retries: 2  # Retry attempts for failed fetches
      min_content_length: 500  # Minimum chars for valid content
      max_content_length: 50000  # Maximum chars to extract per page
      cache_enabled: true  # Enable content caching
      cache_ttl_seconds: 3600  # Cache TTL (1 hour)

  fact_checker:
    enabled: true
    model: analytical  # Use analytical model for fact checking
    verification_level: strict
    enable_contradiction_detection: true

  reporter:
    enabled: true
    model: complex  # Use complex model for final synthesis
    default_style: default
    citation_style: APA
    enable_grounding_markers: true
    use_semantic_extraction: true
    # Structured generation (NEW - guarantees JSON structure, not markdown syntax!)
    enable_structured_generation: true  # Use Pydantic models + programmatic rendering
    structured_generation_fallback: false  # CHANGED: Enable fallback to prevent "Error: Structured generation failed" messages when rate limited
    # Table validation MUST stay enabled - structured generation doesn't validate markdown!
    enable_table_validation: true  # POST-PROCESS to fix missing separator rows
    enable_llm_self_correction: true  # LLM correction is overkill, validation is enough
    max_correction_retries: 3  # CHANGED: More retry attempts before falling back (was 1)

    # Generation Mode: section_by_section (default) or hybrid (multi-pass)
    generation_mode: hybrid  # Safe default - change to 'hybrid' to enable new mode
    fail_on_empty_observations: true  # Raise error if observation filtering produces empty sections
    max_concurrent_blocks: 2  # Throttle for concurrent structured output calls

    # NEW: Hybrid Multi-Pass Report Generation Configuration
    hybrid_settings:
      # Async and concurrency settings
      enable_async_blocks: false  # Enable async table generation (test in staging first)
      fallback_on_empty_observations: true  # Auto-fallback to section-by-section on errors

      # Table anchor configuration
      table_anchor_format: "[TABLE: {id}]"  # Placeholder format in Phase 2

      # Observation selection for calculation context (Phase 1)
      calc_selector_top_k: 60  # Top-scoring observations for calculation prompt
      calc_recent_tail: 20  # Recent observations appended after scoring
      max_calc_prompt_chars: 60000  # Conservative limit: model max 131K tokens, leaving room for template + response

      # Table fallback configuration
      table_fallback_max_rows: 6  # Maximum rows in bullet fallback summaries
      enable_table_fallback_summary: true  # Generate bullet summaries when tables fail

      # Content sanitization
      enable_file_reference_filter: true  # Remove file/tool references from observations
      contamination_patterns:  # Regex patterns for content sanitization
        - '\b[\w\-]+\.(xlsx|xlsm|csv|json|pdf|doc|docx|xls)\b'
        - 'github\.com[/\w\-\.]*'
        - 'gitlab\.com[/\w\-\.]*'
        - '\b(spreadsheet|repository|download|attachment)\b'

      # Timeouts
      holistic_timeout_seconds: 240  # Timeout for Phase 2 holistic report generation

    # NEW: Cell-Level Derivation Tracking for Tables
    table_generation:
      require_cell_derivation: true  # Force LLM to justify every value (reduces hallucination)
      allow_estimated_cells: true    # Allow estimates with clear basis/assumptions
      log_derivation_details: true   # Log all derivations for audit trail
      validate_extractions: true     # Verify "extracted" claims against research content

# Search Configuration
search:
  providers:
    brave:
      enabled: true
      # api_key loaded from BRAVE_API_KEY environment variable
      max_results: 20
      rate_limit: 1
      timeout: 30
    tavily_search:
      enabled: false
      # api_key loaded from TAVILY_API_KEY environment variable
      max_results: 10
      rate_limit: 1
      timeout: 30
  max_concurrent_searches: 3
  batch_delay_seconds: 1.0
  max_results_per_query: 20
  enable_safe_search: true

# Workflow Configuration
workflow:
  max_research_loops: 2
  max_fact_check_loops: 2
  max_total_steps: 200  # Increased for complex multi-country workflows
  max_wall_clock_seconds: 2400  # Increased to 40 minutes for very complex queries
  recursion_limit: 500
  enable_background_investigation: true
  enable_human_feedback: false
  auto_accept_plan: false
  enable_circuit_breakers: true
  enable_progress_tracking: true

  # Async Architecture (Phase 3 - Hybrid Async Implementation)
  # All agent nodes are now async for proper LLM await handling
  # Context-aware streaming bridge automatically detects FastAPI (async) vs MLflow (sync)
  experimental_async_nodes: true  # Feature flag for async agent nodes (set to false to rollback)

# Rate Limiting Configuration - Comprehensive System
#
# CONFIGURATION STRATEGY:
# - This config is tuned CONSERVATIVELY for production (avoid 429 errors)
# - Sequential execution (max_concurrent_per_endpoint: 1) prevents rate limits
# - For faster execution or testing, see RATE_LIMITING_CONFIGURATION.md
# - Increase max_concurrent_per_endpoint to 2-3 for better parallelism if not hitting limits
#
# TIMEOUT BEHAVIOR:
# - Streaming timeouts are AUTO-CALCULATED from workflow.max_wall_clock_seconds
# - With max_concurrent: 1 (sequential), complex queries may need longer max_wall_clock
# - With max_concurrent: 3 (parallel), same queries complete in less time
rate_limiting:
  enabled: true  # Master switch for entire rate limiting system

  # Retry Strategy (exponential backoff with jitter) - TUNED FOR 429 ERRORS
  retry:
    base_delay_seconds: 15.0  # CHANGED: Start with 15s delay (was 5.0) - respect 429 cooldowns
    max_delay_seconds: 120.0  # CHANGED: Cap at 120s (was 30.0) - allow longer exponential backoff
    max_retries: 8  # CHANGED: Up to 8 retry attempts (was 5) - more patience
    jitter: 0.5  # CHANGED: Â±50% randomization (was 0.3) - prevent synchronized retries

  # Request Coordination (prevent bursts) - SERIALIZE TO PREVENT 429s
  # NOTE: Value of 1 serializes all requests (safe but slow)
  #       Increase to 2-3 for better performance if not hitting rate limits
  coordination:
    max_concurrent_per_endpoint: 2

  # Cooldown After 429 Errors - LONGER WAITS
  cooldown:
    default_cooldown_seconds: 15
    respect_retry_after_header: true  # Use Retry-After header if present

  # Token Budget Tracking
  token_tracking:
    enable_sliding_window: true  # Use sliding window for token counting
    window_seconds: 60  # Track tokens over 60s window
    safety_margin: 0.9  # Use only 90% of declared limit

  # Phase Delays (strategic pauses between pipeline stages) - MORE SPACING
  phase_delays:
    after_research_before_fact_check: 8.0  # CHANGED: 8s pause after research (was 3.0)
    after_fact_check_before_report: 12.0  # CHANGED: 12s pause before reporting (was 5.0)
    between_section_generations: 5.0  # CHANGED: 5s between each report section (was 2.0) - CRITICAL for preventing 429s during report generation

# Tier Fallback Configuration
tier_fallback:
  enable_cross_tier_fallback: false  # Allow falling back to lower tiers when rate limited
  fallback_chain:
    complex: analytical      # Complex can fall back to analytical
    analytical: simple       # Analytical can fall back to simple
    simple: micro           # Simple can fall back to micro
    micro: null             # Micro has no fallback (last resort)


# Grounding and Factuality Configuration
grounding:
  enable_grounding: true
  verification_level: strict
  enable_contradiction_detection: true
  factuality_threshold: 0.8

# Entity Validation Configuration
entity_validation:
  enabled: true
  validation_mode: strict  # strict, moderate, lenient
  enable_synthesis_validation: true
  enable_observation_validation: true
  enable_section_validation: true
  track_violations: true

# Reflexion Configuration
reflexion:
  enable_reflexion: true
  reflection_memory_size: 400

# Streaming Configuration
streaming:
  enable_streaming: true
  max_events_per_second: 20
  batch_events: false  # Disabled for real-time event delivery
  batch_size: 1        # Send events individually  
  batch_timeout_ms: 50 # Minimum allowed value (but batching is disabled)

# Citations Configuration
citations:
  citation_style: APA
  enable_citations: true
  max_citations_per_section: 30

# Report Configuration
report:
  default_style: default  # Enable adaptive structure by using DEFAULT style
  enable_grounding_markers: true
  citation_style: APA
  max_table_observations: 80  # Maximum observations for table sections (reduced for hybrid mode)
  max_paragraph_observations: 40  # Maximum observations for paragraph sections (reduced for hybrid mode)

  # Section-specific observation limits (optional overrides for hybrid mode)
  section_observation_limits:
    "Comparative Analysis": 120  # Example: allow more observations for comparison-heavy sections

# Quality Enhancement Configuration
quality_enhancement:
  enabled: true
  detect_redundancy: true
  eliminate_redundancy: true
  optimize_structure: true

# Adaptive Structure Configuration
adaptive_structure:
  enable_adaptive_structure: true  # Disabled - causes section dependency issues
  adaptive_structure_cache_ttl: 3600  # Cache structures for 1 hour
  use_llm_for_structure: true  # Use LLM to generate creative section names
  max_structure_sections: 12  # Maximum number of sections
  min_structure_sections: 4  # Minimum number of sections

# Memory and State Management Configuration
memory:
  max_observations: 280  # Global observation limit (reduced from 400 for hybrid mode efficiency)
  max_observations_per_step: 120  # Per-step observation limit (reduced from 200)
  max_search_results: 400  # Search result accumulation limit
  relevance_buffer: 40  # Minimum high-scoring observations per section before chronological pruning

