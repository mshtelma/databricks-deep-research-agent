# LangGraph Research Agent Configuration
# This file contains all agent configuration parameters including LLM settings,
# research behavior, rate limiting, and tool configurations.

# Model Configuration with per-model parameters
models:
  default:
    endpoint: "databricks-gpt-oss-120b"
    temperature: 0.7
    max_tokens: 4000
    
  query_generation:
    endpoint: "databricks-gpt-oss-20b"
    temperature: 0.5  # Lower temperature for more focused queries
    max_tokens: 2000
    
  web_research:
    endpoint: "databricks-gpt-oss-20b"
    temperature: 0.7
    max_tokens: 4000
    
  reflection:
    endpoint: "databricks-gpt-oss-120b"
    temperature: 0.8  # Higher temperature for creative reflection
    max_tokens: 3000
    
  synthesis:
    endpoint: "databricks-gpt-oss-120b"
    temperature: 0.7
    max_tokens: 6000  # More tokens for comprehensive synthesis
    
  embedding:
    endpoint: "databricks-gte-large-en"
    # No generation parameters needed for embedding

# Research Behavior Configuration  
research:
  max_research_loops: 1
  initial_query_count: 2
  enable_streaming: true
  enable_citations: true
  timeout_seconds: 30
  max_retries: 3
  search_provider: "brave"  # Options: "tavily" or "brave"

# Streaming Configuration
streaming:
  enabled: true              # Global streaming enable/disable
  chunk_size: 50             # Characters per streaming chunk
  word_boundary: true        # Respect word boundaries when chunking
  buffer_size: 1024          # Internal buffer for streaming
  simulate_for_non_streaming: true  # Simulate streaming for non-streaming LLMs
  
  # Per-model streaming support (when known)
  model_support:
    "databricks-gpt-oss-120b": true       # Supports streaming
    "databricks-gpt-oss-20b": true        # Supports streaming
    "databricks-meta-llama-3-1-70b-instruct": true    # Supports streaming
    "databricks-mixtral-8x7b-instruct": true          # Supports streaming
    "databricks-dbrx-instruct": false     # May not support streaming
    
  # Fallback behavior
  fallback_to_invoke: true    # Use invoke if stream method fails
  min_chunk_length: 10       # Minimum characters before emitting a chunk

# Intermediate Events Configuration
intermediate_events:
  emit_intermediate_events: true          # Enable intermediate action/thought events
  reasoning_visibility: "summarized"      # "hidden", "summarized", "raw"
  thought_snapshot_interval_tokens: 40    # Emit thought snapshot every N tokens
  thought_snapshot_interval_ms: 800       # Or every N milliseconds
  max_thought_chars_per_step: 1000       # Truncate thoughts to this length
  
  # Privacy and redaction controls
  redact_patterns:
    - '\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'  # emails
    - '\b[A-Za-z0-9]{20,}\b'                                 # potential API keys
    - '\bsk-[A-Za-z0-9]{32,}\b'                             # OpenAI-style keys
    - '\bBearer\s+[A-Za-z0-9._-]+\b'                        # Bearer tokens
  
  # Event throttling
  max_events_per_second: 10              # Rate limit for event emission
  batch_events: true                     # Batch multiple events together
  batch_size: 5                          # Events per batch
  batch_timeout_ms: 100                  # Max time to wait for batch

# Rate Limiting and Batch Processing Configuration
rate_limiting:
  max_concurrent_searches: 1      # Max parallel API calls per batch
  batch_delay_seconds: 2.0        # Delay between searches (increase to 3-5 for Brave if rate limited)

# Tool Configurations
tools:
  tavily_search:
    enabled: false
    api_key: "{{secrets/msh/TAVILY_API_KEY}}"  # MLflow secret reference
    base_url: "https://api.tavily.com"
    search_depth: "advanced"
    max_results: 5
    timeout_seconds: 30
    max_retries: 3

  brave_search:
    enabled: true  # Enable to use Brave Search instead of Tavily
    api_key: "{{secrets/msh/BRAVE_API_KEY}}"  # MLflow secret reference
    base_url: "https://api.search.brave.com/res/v1"
    max_results: 5
    timeout_seconds: 30
    max_retries: 3

  vector_search:
    enabled: false  # Enable if vector search is available
    index_name: "main.default.docs_index"
    text_column: "content"
    columns: ["source", "title", "url"]
    k: 5
    timeout_seconds: 30

  python_exec:
    enabled: true
    function_names: ["system.ai.python_exec"]
    timeout_seconds: 30

# Databricks Configuration
databricks:
  workspace_url: null  # Set via DATABRICKS_HOST or DATABRICKS_WORKSPACE_URL env vars
  token: null          # Set via DATABRICKS_TOKEN env var

# Environment-specific Overrides
# These can be used to customize behavior per environment
environments:
  dev:
    research:
      max_research_loops: 1  # Faster for development
    rate_limiting:
      max_concurrent_searches: 1  # Conservative for dev
  
  staging:
    rate_limiting:
      max_concurrent_searches: 2
      batch_delay_seconds: 1.5

  prod:
    rate_limiting:
      max_concurrent_searches: 3  # More aggressive for production
      batch_delay_seconds: 0.5
    research:
      max_research_loops: 3  # More thorough research in prod

# Validation Rules
# These define valid ranges for configuration values
validation:
  models:
    temperature:
      min: 0.0
      max: 2.0
    max_tokens:
      min: 100
      max: 8000
  
  research:
    max_research_loops:
      min: 1
      max: 5
    initial_query_count:
      min: 1
      max: 10
  
  rate_limiting:
    max_concurrent_searches:
      min: 1
      max: 10
    batch_delay_seconds:
      min: 0.0
      max: 10.0

# Schema version for compatibility checking
schema_version: "1.0"
created_date: "2025-08-22"
description: "Centralized configuration for LangGraph Research Agent with batch processing"